{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preetamjumech/Hadoop_Practice/blob/main/Preetam_Saha_B1_Hadoop_WordCount.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwjvirExIQAM"
      },
      "source": [
        "#Hadoop\n",
        "This Notebook has all the codes required to install Hadoop in the Colab VM and execute the a WordCount program using the streaming API <br>\n",
        "The mapper.py and reducer.py programs are available in the authors G-Drive / Github and are downloaded as required<br>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK5y3g-ySDmZ"
      },
      "source": [
        "##Acknowledgements\n",
        "Hadoop Installation from [Anjaly Sam's Github Repository](https://github.com/anjalysam/Hadoop) <br>\n",
        "\n",
        "To get the concept behind map-reduce see [this notebook](https://github.com/Praxis-QR/BDSN/blob/main/Basic_WordCount_Concept.ipynb) <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9bT9M1yvyXG"
      },
      "source": [
        "# 1 Download, Install Hadoop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXFZuorwF25e"
      },
      "source": [
        "# The default JVM(Java Virtual Machine) available at /usr/lib/jvm/java-11-openjdk-amd64/  works for Hadoop\n",
        "# But gives errors with Hive https://stackoverflow.com/questions/54037773/hive-exception-class-jdk-internal-loader-classloadersappclassloader-cannot\n",
        "# Hence this JVM needs to be installed\n",
        "!apt-get update > /dev/null #update os\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null #-qq means dont throw errors on screen, /dev/null means throw errors on junk.I dont wanna see these errors"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now, I have to download the hadoop on the machine on colab VM as a tar.gz file. gz means gun-zip"
      ],
      "metadata": {
        "id": "ReILi6gDm04u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bijZAdD_cBMK",
        "outputId": "d3d6eb74-647e-458f-ab1c-b1a28116bb23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# If there is an error in this cell, it is very likely that the version of hadoop has changed\n",
        "# Download the latest version of Hadoop and change the version numbers accordingly\n",
        "#wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n",
        "#!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz\n",
        "!wget  https://downloads.apache.org/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz\n",
        "# Unzip it\n",
        "# the tar command with the -x flag to extract, -z to uncompress, -v for verbose output, and -f to specify that we’re extracting from a file\n",
        "!tar -xzf hadoop-3.3.2.tar.gz\n",
        "#copy  hadoop file to user/local\n",
        "!mv  hadoop-3.3.2/ /usr/local/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-06 06:23:28--  https://downloads.apache.org/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.95.219, 135.181.214.104, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.95.219|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 638660563 (609M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.2.tar.gz’\n",
            "\n",
            "hadoop-3.3.2.tar.gz 100%[===================>] 609.07M  25.5MB/s    in 25s     \n",
            "\n",
            "2022-11-06 06:23:54 (23.9 MB/s) - ‘hadoop-3.3.2.tar.gz’ saved [638660563/638660563]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### so, I unzipped the gz file and moved it to a directory called user/local"
      ],
      "metadata": {
        "id": "pChyZX4Vnr7z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pjQecX-LetK",
        "outputId": "75dd2123-5a91-40f0-b977-bfe53bc46ec2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls /usr/local"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin   cuda-11\t etc\t_gcs_config_ops.so  include  licensing\tsbin   src\n",
            "cuda  cuda-11.2  games\thadoop-3.3.2\t    lib      man\tshare  xgboost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### So, I checked whether the file is there or not, and yes, the file is there named hadoop-3.3.2"
      ],
      "metadata": {
        "id": "H1133wYPoFm1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh6Dqbbrwqpe"
      },
      "source": [
        "# 2 Set Environment Variables and system parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OUc19ZtcBG5"
      },
      "source": [
        "#To find the default Java path\n",
        "#!readlink -f /usr/bin/java | sed \"s:bin/java::\"\n",
        "#!ls /usr/lib/jvm/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ez4T7Gs3RAn"
      },
      "source": [
        "#To set java path, go to /usr/local/hadoop-3.3.0/etc/hadoop/hadoop-env.sh then\n",
        "#. . . export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/ . . .\n",
        "#we have used a simpler alternative route using os.environ - it works\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"   # default is changed\n",
        "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64/\"\n",
        "# make sure that the version number is as downloaded \n",
        "#os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.3.0/\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.3.2/\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### So, we have set up the java home and hadoop home, this is the part of installation"
      ],
      "metadata": {
        "id": "DrxMW2uUovlz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKOGAmCVhXZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "841e00cc-8d66-4af9-e7b7-c741b3a717fe"
      },
      "source": [
        "!echo $PATH"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDFgpWGLhdhl"
      },
      "source": [
        "# Add Hadoop BIN to PATH\n",
        "# Get the current_path from output of previous command\n",
        "#current_path = '/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin'\n",
        "#current_path = '/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin'\n",
        "#new_path = current_path+':/usr/local/hadoop-3.3.2/bin/'\n",
        "#os.environ[\"PATH\"] = new_path\n",
        "#!echo $PATH\n",
        "\n",
        "current_path = os.getenv('PATH')\n",
        "#new_path = current_path+':/usr/local/hadoop-3.3.0/bin/'\n",
        "new_path = current_path+':/usr/local/hadoop-3.3.2/bin/'\n",
        "os.environ[\"PATH\"] = new_path"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $PATH"
      ],
      "metadata": {
        "id": "x_yQ05KezXXt",
        "outputId": "5ff93dec-dab1-42eb-8b2b-f7ba0c5b207a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/usr/local/hadoop-3.3.2/bin/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# so, previously, hadoop was not there in the path, but now it is there, so we are in the coorect path where the hadoop exists. "
      ],
      "metadata": {
        "id": "44JeLPUOp-E3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj00rPPZyEWZ"
      },
      "source": [
        "# 3 Test Hadoop Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhf-zK7NcBDF"
      },
      "source": [
        "#Running Hadoop - Test RUN, not doing anything at all\n",
        "#!/usr/local/hadoop-3.3.0/bin/hadoop\n",
        "# UNCOMMENT the following line if you want to make sure that Hadoop is alive!\n",
        "!hadoop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### That means hadoop exists.this is a check whether hadoop is there or not"
      ],
      "metadata": {
        "id": "tPrLdA9qq-Zr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n3I6iqjGod-"
      },
      "source": [
        "# Testing Hadoop with PI generating sample program, should calculate value of pi = 3.14157500000000000000\n",
        "# pi example\n",
        "#Uncomment the following line if  you want to test Hadoop with pi example\n",
        "# Final output should be : Estimated value of Pi is 3.14157500000000000000\n",
        "#!hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar pi 16 100000\n",
        "!hadoop jar /usr/local/hadoop-3.3.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.2.jar pi 16 100000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### so, hadoop program does its works and it calculates the value of pi."
      ],
      "metadata": {
        "id": "LOqvh2Firv0X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC9WSzgMprwr"
      },
      "source": [
        "# 4 Run WordCount with Hadoop\n",
        "Instead of using Java for Map and Reduce methods, we use the streaming API of Hadoop and two simple python programs as mapper.py and reducer.py\n",
        "\n",
        "This API(Application program interface) allows us to run hadoop from non-java languages because we are going to write our codes in python. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxloM8fzqORx"
      },
      "source": [
        "# get mapper.py reducer.py from G_drive\n",
        "#!gdown https://drive.google.com/uc?id=1VTzQ18cWAj6L29ncW6sABy-ITmDCcv5r\n",
        "#!gdown https://drive.google.com/uc?id=1Or8Cbf9AsFMHStjMzDw3pXCd6TZ0dqxJ\n",
        "\n",
        "#get mapper.py reducer.py from this git repository\n",
        "!wget -q https://raw.githubusercontent.com/Praxis-QR/BDSN/main/mapper.py\n",
        "!wget -q https://raw.githubusercontent.com/Praxis-QR/BDSN/main/reducer.py"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mapper generates a key-value pair, where the key being the word, and the value always 1. \n",
        "### and the reducer counts "
      ],
      "metadata": {
        "id": "wQpOeSU3ttkB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRWPIMj9qpZK"
      },
      "source": [
        "# to see the codes, uncomment the following lines\n",
        "#!cat mapper.py\n",
        "#print(\"\\n----------------------    see above for mapper, see below for reducer\")\n",
        "#!cat reducer.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MQ_4eaxqlCW"
      },
      "source": [
        "# python codes are made executable\n",
        "!chmod u+rwx /content/mapper.py\n",
        "!chmod u+rwx /content/reducer.py"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### so, we have  changed mod to make them executable."
      ],
      "metadata": {
        "id": "yCxm_VNduISF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yfxo4CJrCtJ"
      },
      "source": [
        "# get a simple txt file as data for word count\n",
        "# or you can upload your own\n",
        "#!gdown https://drive.google.com/uc?id=1R5W0UVH2S3JjPxerqyX4ue5y6tMt0Wkk\n",
        "!wget -q https://raw.githubusercontent.com/Praxis-QR/BDSN/main/Chronotantra.txt"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### so, we got the input file."
      ],
      "metadata": {
        "id": "Gouk_nWnuWf-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W38-6u0KrSrp",
        "outputId": "4ff9cb2c-4e3f-406a-b8be-f0289fb4889e"
      },
      "source": [
        "# locate the streaming jar file\n",
        "!find / -name 'hadoop-streaming*.jar'"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.3.2/share/hadoop/tools/lib/hadoop-streaming-3.3.2.jar\n",
            "/usr/local/hadoop-3.3.2/share/hadoop/tools/sources/hadoop-streaming-3.3.2-sources.jar\n",
            "/usr/local/hadoop-3.3.2/share/hadoop/tools/sources/hadoop-streaming-3.3.2-test-sources.jar\n",
            "find: ‘/proc/42/task/42/net’: Invalid argument\n",
            "find: ‘/proc/42/net’: Invalid argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### so, we found the path of streaming API"
      ],
      "metadata": {
        "id": "-8JXqqadurXs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h60r05EPk-xF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f9b7c21-e54b-4537-a6cb-3ccffc0f9ce3"
      },
      "source": [
        "# remove output directories\n",
        "!rm -r wc_out\n",
        "!rm -r wc2_out"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'wc_out': No such file or directory\n",
            "rm: cannot remove 'wc2_out': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hadoop and spark never overwrite directories. So, it should not exist. If it exists, hadoop will throw a huge error. "
      ],
      "metadata": {
        "id": "NdT-YJ09u9a8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### so, we are ready for our hadoop program, it needs four things,1. input file, 2. output directory, 3. mapper 4. reducer"
      ],
      "metadata": {
        "id": "yxGA8hH4wuVr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH-I6ow7rl9k"
      },
      "source": [
        "# execute the streaming jar with proper parameters\n",
        "# four parameters are input file, output directory, the mapper progra, the reducer program\n",
        "#\n",
        "#!hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar -input /content/hobbit.txt -output /content/wc_out -file /content/mapper.py  -file /content/reducer.py  -mapper 'python mapper.py'  -reducer 'python reducer.py'\n",
        "#!hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar -input /content/Chronotantra.txt -output /content/wc_out -file /content/mapper.py  -file /content/reducer.py  -mapper 'python mapper.py'  -reducer 'python reducer.py'\n",
        "#!hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar -input /content/Chronotantra.txt -output /content/wc_out  -mapper 'python mapper.py'  -reducer 'python reducer.py'\n",
        "!hadoop jar /usr/local/hadoop-3.3.2/share/hadoop/tools/lib/hadoop-streaming-3.3.2.jar -input /content/Chronotantra.txt -output /content/wc_out  -mapper 'python mapper.py'  -reducer 'python reducer.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###, so, the hadoop worked succesfully but in a single machine cluster, if it is a multi- machine clusters, the code will remain same but the installation will be more complex. "
      ],
      "metadata": {
        "id": "qTgIKNjpxkmd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d2DTo_GsBot",
        "outputId": "ff43f0e4-faa1-4bb4-db3d-f74c5392e7f3"
      },
      "source": [
        "# check output directory\n",
        "!ls wc_out"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000  _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### hadoop generates two files, one is success file and another one is the actual output file. if success file is not there, there is failure somewhere."
      ],
      "metadata": {
        "id": "xco-ycOO0jUc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91LjuJnlsKGz",
        "outputId": "7e9a5280-981a-4b8f-dbf7-85ae760e3f46"
      },
      "source": [
        "# see actual output\n",
        "#!tail wc_out/part-00000\n",
        "!head wc_out/part-00000"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t8\n",
            "10\t2\n",
            "100\t2\n",
            "1000\t1\n",
            "105\t1\n",
            "108\t2\n",
            "109\t1\n",
            "11\t1\n",
            "110\t2\n",
            "113\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### this is the head of our output file"
      ],
      "metadata": {
        "id": "jtGl_IU11aR9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbaMQ1lWpB7O"
      },
      "source": [
        "### Sorting the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld-kLgmOpLw8"
      },
      "source": [
        "#https://www.geeksforgeeks.org/sort-command-linuxunix-examples/\n",
        "!sort -nr -k 2 -t$'\\t' wc_out/part-00000 > sorted.txt"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K09MH8h4qNfM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09dcd005-a772-416d-d9d9-f9d9f6aafb7a"
      },
      "source": [
        "!head -30 sorted.txt"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "would\t346\n",
            "could\t247\n",
            "one\t198\n",
            "time\t156\n",
            "like\t145\n",
            "know\t144\n",
            "us\t134\n",
            "mars\t119\n",
            "back\t106\n",
            "even\t105\n",
            "world\t97\n",
            "something\t95\n",
            "see\t95\n",
            "well\t93\n",
            "hermit\t93\n",
            "two\t87\n",
            "people\t86\n",
            "course\t84\n",
            "around\t84\n",
            "way\t82\n",
            "first\t80\n",
            "really\t79\n",
            "new\t76\n",
            "little\t74\n",
            "long\t73\n",
            "still\t71\n",
            "information\t70\n",
            "ai\t67\n",
            "good\t63\n",
            "earth\t60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZN0vwE2pxHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "079f2b54-99f9-4232-9258-5e2e3c92c774"
      },
      "source": [
        "!tail -30 sorted.txt"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2150\t1\n",
            "214\t1\n",
            "206\t1\n",
            "205\t1\n",
            "2019\t1\n",
            "2018\t1\n",
            "2007\t1\n",
            "20062007\t1\n",
            "2000\t1\n",
            "1999\t1\n",
            "1970s\t1\n",
            "1956\t1\n",
            "187\t1\n",
            "186\t1\n",
            "17866\t1\n",
            "156\t1\n",
            "155\t1\n",
            "15\t1\n",
            "150\t1\n",
            "1493\t1\n",
            "133\t1\n",
            "132\t1\n",
            "12th\t1\n",
            "12700\t1\n",
            "115\t1\n",
            "113\t1\n",
            "11\t1\n",
            "109\t1\n",
            "105\t1\n",
            "1000\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "print('signed off at  ',datetime.now(pytz.timezone('Asia/Kolkata')))"
      ],
      "metadata": {
        "id": "kO8mrX8_0BS-",
        "outputId": "06b9093f-c83b-4c08-891c-eac9b55bb363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "signed off at   2022-11-06 13:00:04.247851+05:30\n"
          ]
        }
      ]
    }
  ]
}